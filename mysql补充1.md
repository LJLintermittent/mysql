### 视图

在mysql里有两个视图:

1.一个是view，他是一个用查询语句定义的虚拟表，在调用的时候执行查询语句并生成结果，创建视图的语句时create view，而它的查询方法与表一样

2.另一个是innodb在实现mvcc时用到的一致性视图，consistant read view，用于支持RC,RR这两种隔离级别的实现

mvcc中的一致性视图并没有物理结构，作用是事务执行期间用来定义"我能看到什么数据"

### mvcc

在可重复读隔离级别下，事务在启动的时候就拍了个快照，注意：这个快照是基于整个库的

在innodb中每个事务都有一个唯一的事务ID，它在事务开始的时候向innodb的事务系统来申请，在申请的时候是按顺序严格递增的

而每行数据也是有多个版本的，每次事务在更新行记录的时候，都会生成一个新的数据版本，并且把事务id赋给这个数据版本的事务id，即为row_trx_id，同时，旧的版本数据要保留，并且在新的数据版本中，能够有信息直接能拿到它

也就是说数据表中的一条记录，有多个版本，每个版本有自己的row_trx_id

语句更新会生成undo log（回滚日志），所谓的undo log就是带事务id的更新语句 指向某一个版本数据

根据可重复读的定义：一个事务启动以后，能够看到所有已经提交的事务结果，但是在之后事务进行的期间，其他事物对数据的操作结果对这个事务内是不可见的

因此MVCC形象的说就是一个事务启动一个，告诉数据库，以我这个启动时刻为准，只有早于我这个时刻的数据版本，我才可以看到，如果是我启动以后才生成的，就不认，当然，如果上一个版本也不可见，那么就继续往前寻找。

实际上，innodb为每一个事务构造了一个数组，用来保存这个事务启动瞬间，当前正在活跃的所有事务id，活跃的意思就是这个事务启动了但是还没提交，数组里面的事务ID的最小值记为低水位，当前系统里面已经创建过的事务ID的最大值+1记为高水位，这个视图数组和高低水位，就组成了当前事务的一致性视图，而数据版本的可见规则，就是基于数据的row_trx_id和这个一致性视图的对比结果得到的。

innodb利用了所有数据都有多个版本这个特性，实习了秒级创建快照的能力

一个数据版本，对于一个事务视图来说，除了自己的更新总是可见以后，有三种情况：

1.版本未提交，不可见

2.版本已提交，但是在视图创建后创建，不可见

3.版本已提交，且在视图创建前创建，可见

事务的可重复读的能力是怎么实现的？

可重复读的核心是一致性读，而事务更新时，必须是当前读，如果当前记录的行锁被其他事务占用，就需要等待锁

* 在可重复读级别下，只需要在事务开始的时候创建一致性视图，之后事务里的其他查询都会使用这个一致性视图
* 在RC下，每一个语句执行前都会重新计算一个视图

上面说的视图都是mvcc多版本并发控制时用到的一致性视图

关于mvcc这块总结起来：innodb的行数据会有多个版本，每个数据版本都有自己的row_trx_id，每个事务或者语句都有自己的一致性视图，对于innodb，普通查询语句是一致性非锁定读，一致性的意思是会根据row_trx_id和一致性视图来确定每个数据的版本，从而找出可见的版本数据，非锁定指的是由于读取的是快照数据，所以不需要上锁，因为没有事务需要对历史版本的数据进行一个操作，除此以外，可以认为mvcc是借助undolog来实现的快照版本，所以一致性读并不会对资源产生什么压力，因为undolog本身就会记录下这些快照版本

对于可重复读，查询只承认在事务启动前就已经完成提交的数据

对于读已提交，查询只承认在事务中的每一个操作前的已经完成提交的数据

这是RR与RC的主要区别与实现原理，都是通过mvcc，只不过计算一致性快照或者叫做一致性视图的时间不一致而已

当前读是指总是读取已经提交完成的最新版本

~~~wiki
innodb要保证一个规则，事务启动前还没有提交的事务，他都不可见
但是只保存一个已经提交的事务的最大事务id是不够的，因为那些比最大值小的事务，之后也可能更新，每个事务完成的时间不一样，所以
事务启动的时候还要保存 现在正在执行的事务id列表，如果一个row_trx_id在这个事务id列表中，那么他应该也是不可见的
~~~

### 唯一索引和普通索引

唯一索引和普通索引在查找时的性能差距微乎其微，以这样的sql语句为例，select value from  t where key = x；

对于普通索引，他找到第一个key  = x的节点后，会继续向下查找，因为对于普通索引，没有唯一性约束，必须找到key不为x的第一个记录才停止

对于唯一索引，由于限定为唯一性约束，那么只要找到记录，就会停止检索

特别注意一点，在innodb存储引擎中，数据的读取是按页为单位来读取的，也就是说不管是通过主键索引在叶子节点直接找到整行记录还是先通过普通索引查找到叶子节点的主键，或者在查找的过程中直接命中记录，这种叫覆盖索引，还是找到主键以后回表查询，在主键索引树上继续查找整行记录，记住，无法是哪一种情况，在索引上查找完以后都不会直接读取到数据，数据读取以页为单位，当读取到一条记录时，并不会直接把数据从磁盘中读取出来，而是以页为单位，将其整体读取内存，在内存中继续快速的定位所需的数据，每个数据页的大小默认是16K。所以在查询中，普通索引和唯一索引的性能差距是微乎其微

### change buffer

在说明普通索引和唯一索引对更新语句的性能影响时，需要用到change buffer概念

当需要对一个数据页中的某个数据进行更新操作时，如果数据页在内存中，那么直接在内存中进行更新，如果这个数据页还没有在内存，在不影响数据一致性的前提下，innodb会将这些更新操作缓存在changebuffer中，这样就不需要从磁盘中读取这个数据页，在下次查询的时候访问这个数据页，将数据页要必须加入内存，然后执行changebuffer中与这个页有关的操作，对数据进行更新，通过这种方式可以既保证了数据的逻辑正确性，同时还减少了更新时读取磁盘的次数，可以有效提升性能

同时虽然名字叫change buffer，为了保证更新一定能成功，change buffer可以持久化到磁盘

将changebuffer中的操作应用到数据页，得到最新结果的过程称为merge，那么访问数据页将其加到内存时必定要merge的，除此以外，后台线程会定期merge，在数据库正常关闭的时候，也会触发merge

显然能够将更新操作先记录到change buffer，减少磁盘读写，语句的执行速度会得到明显提升，而且，数据读入内存要占用buffer pool缓冲池，所以这种方式还能避免内存过多的占用，提高内存利用率

对于唯一索引来说，所有的更新操作都要先判断这个操作是否违反了唯一性约束，比如插入一条记录，要先判断表中是否已经存在这个key(4,400),而这个判断必须要把数据页读取内存，如果都已经读入内存了，那么更新也就不需要用到change buffer

其实只有普通索引能用到change buffer

changebuffer使用的是buffer pool缓冲池中的空间，所以它的大小是受制于buffer pool的。

接下来对比两种情况下普通索引与唯一索引对于操作的性能差距

第一种情况，如果需要更新的数据所在的数据页在内存中：

对于唯一索引，在内存中进行唯一性约束判断，然后更新，最后结束语句

对于普通索引，在内存中直接更新，结束语句

对于第一种情况，唯一索引比普通索引多了一次判断唯一性的操作，这个其实不能体现差距，对于CPU来说这是一件很轻松的事情

第二种情况，如果需要更新的数据所在的数据页不在内存中：

对于唯一索引，由于需要判断唯一性约束，需要从磁盘中读取行记录所在的数据页到内存中，由于已经到了内存，然后进行更新，结束。

对于普通索引，数据块不在内存中时，会直接在changebuffer中做更新操作，然后结束语句

可以看出，区别就在于唯一索引比普通索引多了一次磁盘IO，而磁盘IO是对于数据库来说成本最高的操作之一，change buffer因为减少了随机磁盘访问，所以在业务表面临大量并发更新需求时，可以有效地提升性能

change buffer的使用场景：

因为merge操作是真正进行数据更改的时刻，而changebuffer的主要目的是将更新操作缓存下来，所在在一个数据页在做merge前，changebuffer记录的数据应该越多那么收益就越大

因此对于写多读少的业务场景，在changebuffer写完以后马上被用的概率是很低的，这时候changebuffer的效果最好，这种业务模型对应的是大型系统中的日志服务处理类微服务和账单类微服务

如果一个业务在更新后马上就要做查询，那么即使满足了条件，还是立马要进行merge，即把数据页读取到内存中，这样磁盘IO的次数并不会减少，反而增加了维护changebuffer的代价，起了副作用

由于changebuffer在bufferpool中，也就是内存空间，那么如果发生断电，更新缓存没有merge是很危险的，所以changebuffer一定是可持久化的存放在ibdata表空间中

### 优化器的优化逻辑

优化器选择索引的目的是找到一个最优的执行方案，并用最小的代价去执行语句，在数据库层面，扫描行数是影响优化器判断执行代价的因素之一，扫描行数越少，意味着需要访问磁盘的数据更少，同时如果是普通索引，那么扫描行数少还意味着从普通索引查到叶子节点拿到主键后再去主键索引树回表查询的次数更少

当然扫描行数并不是唯一的判断标准，优化器还会结合是否使用临时表，是否排序等因素进行综合判断

在mysql真正执行sql语句之前，并不能精确的知道满足这个条件的记录有多少条，只能根据统计信息来进行判断，统计信息就是索引的区分度，一个索引列上不同的值越多，区分度就越高，而一个索引上不同值的个数，称为基数，在mysql中叫做cardinality

只不过这个cardinality的值是不准确的，他是通过采样统计计算出来的，所以在线上环境选错索引有可能是因为基数统计信息出错导致的，可以使用analysis table来重新统计

### 由字符串的索引设置场景引出前缀索引

如果项目中涉及邮箱注册登录的业务，那么一定会使用的where条件后面跟邮箱地址的查询语句，这个时候如何对邮箱这个较长的字符串字段建立索引是一个值得分析的事情

如果邮箱字段上面没有索引，只能做全表扫描。mysql是支持的前缀索引的，也就是说可以定义一个字符串的一部分作为索引，如果创建索引的时候不指定索引的长度，那么索引就会包含整个字符串。

第一种情况，如果使用默认的全长索引，那么步骤应该是首先在普通索引树上进行查找，最终找到叶子节点的主键，然后拿着主键去主键索引表查找聚集索引的叶子节点的整行记录，判断where条件是否正确，最终将记录加入结果集，然后因为是普通索引，还要回到普通索引树上查看刚才这个位置的下一条记录是否满足where条件，如果不满足，将结果集返回

第二种情况；如果使用字符串前缀索引，首先从普通索引树上找到满足索引值是以xxx开头的记录，找到主键，拿主键去主键索引树查找行，判断全量值是否相等，也就是以xxx开头的这个记录所对应的主键在主键索引找到的整行记录中的值是否与where条件值相等，如果相等，加入结果集，如果不相等，回到刚才普通索引树中的位置，进行下一个位置的查找，同样拿主键去主键索引树，拿整行记录中的索引列的值与where条件的值进行比较。

根据上面的比对，可以看出来通过前缀索引可能存在索引树搜索次数变多的情况。

使用前缀索引， 定义好长度，就可以节省空间又不用多出额外的存储空间

那么问题是如何确定一个字符串的前缀索引应该取多长呢？

建立前缀索引关注最多的就是区分度，也就是在这张表中这个字段不同的行个数，cardinality基数，我们可以通过

select count(distinct left(email,4)) as L4;

select count(distinct left(email,5)) as L5;

select count(distinct left(email,6)) as L6;这种方式来查看不同长度下索引列不同的个数，从而计算出区分度

### 关于语句执行流程架构

客户端-》连接器-》查询缓存-》返回结果

客户端-》连接器-》分析器-》优化器-》执行器-》存储引擎

执行语句要先连接数据库，这是连接器的工作，当一个表上有更新操作时，跟这个表有关的查询缓存就会失效，一般不建议使用查询缓存，并且在mysql8.0以后也确实移除了查询缓存这个功能

接下来分析器会经过词法分析和语法分析来解析sql语句的语法格式是否正确，然后到了优化器来选择使用哪个索引，然后执行器负责具体执行，执行是使用的存储引擎提供的读写接口

如果每一次更新操作都需要写一次磁盘，然后磁盘找到对应的记录，然后在更新，整个过程的io成本很高，所以使用redo先写日志，等不忙的时候再写磁盘，既能将多次更新用一次访问磁盘来解决，还可以为事务提供原子性和持久性支持，当刷盘的时候失败了，可以通过redo来进行crash-safe恢复

更新语句涉及两个重要模块，redo log 和binlog，这块涉及一个名词，WAL write ahead logging，它的关键就是先写日志再写磁盘

具体来说，当有一条记录需要更新的时候，innodb存储引擎会先把记录写到redobuffer，并更新内存（缓冲池中的数据页）更新完以后由于数据页比磁盘中的页新，所以叫脏页，这个时候更新就算完成了，同时innodb引擎会在适当的时候将这些操作落盘，更新操作不仅可能要写changebuffer，还必须要写redologbuffer

redolog大小是固定的，比如可以配置一组四个文件，每个文件的大小是1G，那么redolog总共可以存放4G的操作数据

当redo满了以后需要清理一部分，将一部分落盘，这块有一个技术checkpoint，用来在缓冲池不够的时候，将脏页落盘的

有了redo log，innodb就可以保证即使数据库发生宕机，之前提交的记录也不会丢失，这个能力被称为crash-safe，redolog提供了对innodb存储引擎原子性和持久性的支持，隔离性是由锁来保证，一致性是由undolog来保证

其实重做日志，也有自己的重做日志缓冲，所以应该是先写到重做日志缓冲中，然后由缓冲落盘写到redolog日志文件中

redolog是innodb引擎特有的日志，而server层也有自己的日志，称为binlog(归档日志)

因为mysql最开始只有myisam引擎，myisam没有crash-safe能力，binlog日志只能用于归档，而innodb是以一种插件的形式引入mysql，既然只依靠binlog没有crash-safe能力，那么innodb就自己引入了一套日志系统，redolog来实现crash-safe能力

这两种日志的主要区别：

1.redo是innodb引擎层所特有的，binlog是mysql共享的

2.redo是物理日志，记录在某个数据页上做了什么修改，binlog是逻辑日志，记录的是这个语句的原始逻辑，比如给id=2这一行的c字段+1

3.redo是循环写的，空间固定 会用完，binlog是追加写的，追加写的意思是binlog文件写到一定大小后会切换到下一个，并不会覆盖以前的日志

执行update的流程：

1.执行器先找存储引擎取id=2这一行，id是主键，存储引擎调用接口直接在聚集索引树上找到叶子节点，就是整行记录，如果id=2这一行记录本身就在内存中，就直接从内存中返回给执行器，否则，就需要从磁盘中读入内存，然后再返回，总之存储引擎找到以后要返回给执行器

2.执行器拿到存储引擎给的数据，对这个记录进行修改后，再交给存储引擎，交给它让他写入这个新数据

3.存储引擎将新纪录的数据页更新到内存中，同时将这个更新操作记录到redolog里面，此时redo处于prepare状态，然后告诉执行器已经执行完成了，随时可以提交事务

4.执行器生成这个操作的binlog，并把binlog写入磁盘

5.执行器调用存储引擎的事务提交接口，引擎把刚刚写入的redo改为commit状态，更新完成

重点就是把redo的写入拆分成了两个步骤：prepare和commit，这就是二阶提交

二阶提交的目的是让两份日志之间的逻辑一致。

为什么用二阶提交？反证

由于redo和binlog是两个独立的逻辑，如果不用二阶提交，要么是先写完redo，再写binlog，要么是先写完binlog，再写redo

1.如果先写redo，再写binlog，假设在写完redo后，宕机了，没有写binlog，那么redo写完后，数据库是可以crash-safe的，仍然可以恢复数据，所以数据实际上是更新了，但是这个时候binlog没写完就宕机了，那么binlog里面是没有记录修改逻辑的，因此，如果使用binlog做备份日志，或者主从复制，就会造成数据丢失更新的情况。

2.先写binlog再写redo log，如果在写binlog写完之后宕机，由于redo没写完，崩溃恢复以后事务无效，所以恢复以后相当于没改，但是binlog里面却记录改了值，之后用binlog来备份，就会多出来一个事务，与原库的记录不同

二阶提交不仅能解决误操作以后恢复的准确性，更能在主从复制的时候保证主从库的数据一致性

redolog有一个参数是把每次事务的提交的日志都立即持久化在磁盘，这个功能建议开启，sync_binlog也建议设置成1，保证每次事务的binlog都持久化到磁盘，保证异常重启后binlog不丢失

二阶提交是一个重要的思想，是跨系统维持数据逻辑一致性的常用方案，即使不涉及数据库开发，在日常开发中也能用到

### sql语句为什么会突然变慢

当内存中的数据页与磁盘中的数据页不一致的时候，就称为脏页，内存数据写入到磁盘后，内存和磁盘上的数据页的内容一致了，称为干净页，不论是干净页还是脏页，都是在内存中的数据页

平时执行的很快的更新操作，如果发生了抖动，其实就是在写日志和内存，也就是刷脏页

以下场景会触发刷脏页逻辑：

1.当redo满了后，这时候系统会阻塞所有更新操作，把checkpoint往前推，redo留出空间可以继续写，checkpoint往前推进的过程对应的是原点与推进后的点之间的空间的日志刷盘，之后从write pos到checkpoint点之间的空间都是可以再次写入，这块为了好理解可以把整个日志的内存空间抽象成一个闭环，checkpoint之前到writepos这个弧上的空间就是未使用空间，其实redobuffer只要了写操作，那么就会比redo文件要新，那么redobuffer中的数据页也是脏页，满了就会刷盘

2.系统内存不足时，当需要在内存中分配新的内存，内存不够用了，要用LRU算法淘汰最近最少使用的数据页，空出一些内存给别的数据页来使用，如果淘汰的数据页是脏页，就要将脏页写到磁盘

3.mysql认为系统空闲时，会找时间刷一点脏页

4.mysql正常关闭时，mysql会把内存中的脏页都刷新到磁盘，这样下次启动时，会从磁盘中读取数据

关于上面四种redo时机的选择对性能的影响：

对于mysql正常关闭和mysql空闲时刷脏页这个其实对性能没什么影响，不解释

1.对于redo满了需要刷脏页，这时候整个系统会无法执行更新操作，这种情况要尽量避免

2.内存不够用了，要先将脏页刷到磁盘减轻内存压力，让数据页可以被读进来，这种情况是常态，innodb使用缓冲池bufferpool来管理内存，缓冲池中的内存页有三种状态：第一种是还没有使用的，第二种是使用了并且是干净页，第三种是使用了并且是脏页

innodb的策略是尽量使用内存，因此对于长时间运行的库，未被使用的页很少。

由于查询也需要使用内存页，如果内存不够，一个查询还很大，要淘汰的脏页个数太多，会导致查询的响应时间变长

如果redo写满，更新全部阻塞，写性能跌到0

所以铺垫了这么多，就是为了说明脏页的刷新时机会对系统的性能造成影响，为此需要了解innodb刷脏页的控制策略，主要是需要明确告诉innodb所在主机的io能力，这样innodb才能知道需要全力刷脏页的时候，可以刷多快

innodb_io_capacity参数，它会告诉innodb你的磁盘能力，这个值建议设置成磁盘的iops

还有一个有趣的策略：如果一个查询突然比平时慢，那么可能是在查询之前内存不够了，需要先刷脏页，而mysql中的另一个机制，可能会让查询变得更慢，就是刷新邻接页，如果在准备刷新一个脏页的时候发现这个脏页旁边的数据页也刚好是脏页，就会把这个邻居一起刷掉，这个找邻居行为在机械磁盘时代是很有意义的，可以减少随机IO，机械硬盘的随机iops一般只有几百，相同的逻辑操作减少了随机IO，意味着系统的性能会得到提高，而如果使用的是SSD这种iops比较高的设备，因为这个时候iops已经不是瓶颈了，那么还不如只是刷掉一个脏页，减少sql的响应时间

在mysql8.0已经默认关闭了刷新邻居页功能

redo最后总结：

redo是关系型数据库的核心，保证了acid中的d，所以redo是一个牵一发而动全身的操作

redo满了，会有很多影响：

1.把相对应的数据页中脏页持久化到磁盘，checkpoint前推

2.由于redo还记录了undo的内容，undo log buffer也需要写入undolog

3.如果设置了每次事务提交不自动提交redo，那么还要把内存中的所有redo持久化到磁盘中。

4.redo还记录了changebuffer，所以还需要把changebuffer purge到ibd

上面每一种操作，都会占用磁盘io，影响DML，这就是数据库偶尔抖动的原因

innodb如何判断一个页是不是脏页？

每个数据页头部都有一个LSN，大小是八个字节，每次更新都会递增，对比这个LSN和checkpoint的LSN，比checkpoint小的一定是干净页

比checkpoint中的LSN大的就是脏页